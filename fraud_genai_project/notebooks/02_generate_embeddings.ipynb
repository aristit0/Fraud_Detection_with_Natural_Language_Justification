{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a620b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5cd03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DistributedEmbedding\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.task.maxFailures\", \"8\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccf5ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Load Hive table\n",
    "df = spark.sql(\"SELECT transaction_id, user_id, amount, category, country FROM datamart.fraud_transactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24191682",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Add 'text' column for embedding input\n",
    "df = df.withColumn(\"text\", concat_ws(\" \", \"user_id\", \"amount\", \"category\", \"country\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2604cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Define function to run inside Spark workers\n",
    "def embed_partition(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import pandas as pd\n",
    "\n",
    "    model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")  # GPU-enabled if available\n",
    "    texts = pdf[\"text\"].tolist()\n",
    "    embeddings = list(model.encode(texts, show_progress_bar=False, batch_size=64))\n",
    "    embedding_cols = [f\"emb_{i}\" for i in range(len(embeddings[0]))]\n",
    "    emb_df = pd.DataFrame(embeddings, columns=embedding_cols)\n",
    "    emb_df[\"transaction_id\"] = pdf[\"transaction_id\"].values\n",
    "    return emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118a427",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Run distributed embedding using mapInPandas\n",
    "schema = \"transaction_id LONG, \" + \", \".join([f\"emb_{i} FLOAT\" for i in range(384)])  # 384 dims for bge-small\n",
    "\n",
    "embedding_df = df.select(\"transaction_id\", \"text\") \\\n",
    "    .repartition(32) \\\n",
    "    .mapInPandas(embed_partition, schema=schema) \\\n",
    "    .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a90e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Write embeddings to HDFS with optimized settings\n",
    "embedding_df.coalesce(4).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(\"hdfs:///tmp/fraud_embeddings_parquet\")\n",
    "\n",
    "print(\"âœ… Embeddings written to HDFS: /tmp/fraud_embeddings_parquet\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
